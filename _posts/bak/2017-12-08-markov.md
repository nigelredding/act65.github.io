---
layout: post
title: Markov Chains
---

**NB**: This is a work in progress. We sketch some ideas about Markov chains and random walks on graphs. I'll be updating this regularly.
I followed [this](http://steventhornton.ca/markov-chains-in-latex/) to make the diagrams in latex.

A sequence of random variables $X_{0},X_{1},\ldots$ taking values in a set of *states* $S \subseteq \\{0,1,2,\ldots\\}$ is said to be a *Markov chain* if

$$
P(X_{n+1} = a_{n+1} | X_{n} = a_n, \ldots, X_{0}=a_0)
= P(X_{n+1} = a_{n+1} | X_{n} = a_{n}) \text{ for all } n \geq 0, a_{n+1}, \ldots, a_0 \in S.
$$

If for any $i,j \in S$ there exists $p_{ij}$ such that

$$P(X_{n+1} = a_{n+1} | X_{n} = a_{n}) = p_{ij}$$

holds for all $n \geq 0$, we say that the Markov chain is *time homogeneous*. We assume that all Markov chains in this blog post are time homogenous. We also assume that $S=\\{0,1,2,\ldots,n\\}$ for some $n \geq 0$. Given a Markov chain, we get a transition matrix $P = (p_{ij}){_{i,j \in S}}$. The matrix $P$ completely determines For our purposes, $S$ will be finite.

Let's do an example. We can represent our Markov chain with a directed graph with labeled edges. Consider the Markov chain with $3$ states with the following transition matrix

$$
P = \begin{pmatrix}
  0 & 1/3 & 2/3 \\
  0 & 1   & 0   \\
  0 & 1/2 & 1/2
  \end{pmatrix}
$$

Then there is the corresponding directed graph with labeled edges

<center> <img src="{{site.baseurl}}/images/tex/markov1.png" width="200"> </center>

Here are some warm up questions
1. Starting at state $0$, what is the probability that we'll be in state $1$ after $3$ steps?
2. Starting at state $0$, what is the probability that we'll be in state $1$ after $50$ steps?
3. What are some formulas for calculating these (types of) probabilities, in a gernal setting (i.e. just using the matrix)?

For question 1, notice that there are two $3$-step paths which start at $0$ and end at $1$.
The two paths are

$$
0 \xrightarrow{1/3} 1 \xrightarrow{1} 1 \xrightarrow{1} 1 \text{ and }
0 \xrightarrow{2/3} 2 \xrightarrow{1/2} 2 \xrightarrow{1/2} 1
$$

The probability that we'll take the first path is $1/3$, and the probability that we'll take the second path is $1/6$. Hence the probability of going from state $0$ to state $1$ in $3$ steps is $1/2$.

For question 2., consider what a path looks like. There are two options:

$$
0 \xrightarrow{1/3} 1 \text{ (loop $49$ times at $1$) }
$$

and

$$
0 \xrightarrow{2/3} 2 \text{ (loop $48$ times at $2$) } \xrightarrow{1/2} 1
$$

The probability that we take the first path is $\frac{1}{3}$. For the second is $\frac{2}{3} \cdot \frac{1}{2}^{48} \cdot \frac{1}{2} =  \frac{1}{3 \cdot 2^{48}}$. So the probability of starting in state $1$ and ending after $50$ steps is $\frac{2}{3} + \frac{1}{3 \cdot 2^{48}} \cong \frac{2}{3}$.

Now let's tackle the third. Remember that we have our matrix $P$. By definition, the probability from going from state $i$ to state $j$ in one step is $p_{ij}$. Let $q_{ij}$ represent this probability. Now what's the probability of going from state $i$ to $j$ in two steps? Well, any path will look like

$$
i \xrightarrow{p_{ik}} k \xrightarrow{p_{kj}} j
$$

and that occurs with probability $p_{ik} p_{kj}$. Hence, we get

$$
q_{ij} = \sum\limits_{k \in S} p_{ik} p_{kj}
$$

which is the $(i,j)$-th entry of $P^2$. More generally, for any $i,j$, the probability of starting at state $i$ and ending at state $j$ after $n$ steps is the $(i,j)$-th entry of $P^n$.

What we're really interested in is the *distribution* of paths through our Markov chain. This is also studied in the continous case where $S=[0,1]$.

Now let $\\{X_n\\}$ be a Markov chain. Let $\pi$ be a row vector in $\mathbb{R}^{\|S\|}$, where $\pi_i$ is the probability of chosing $i$ on our first step. We have $\sum \pi_i = 1$. We say $\pi$ is a *distribution* over $S$ (for obvious reasons). We say that $\pi$ is stationary if $\pi = \pi P$. That is, each $X_n$ has the same distribution.

Suppose the graph of $\\{X_n\\}$ is strongly connected with a set of states $S$ and transition matrix $P$. The *Fundamental Theorem of Markov Chains* states that there is a unique stationary distribution $\pi$ of $\\{X_n\\}$. Since we have $\pi = \pi P$, we also have $\pi = \pi P^n$ for all $n \geq 0$. We can also show that for any distribution $\rho$, the limit

$$
\lim\limits_{n \to \infty} \rho P^n = \pi.
$$

This basically says that if we walk along the graph of $\\{X_n\\}$, the probability that you end up at some vertex $v$ far along into the walk is independent of the starting point.

Let's consider another example. Suppose we have two states, Rainy$=0$ and Sunny$=1$.
Consider the following Markov chain

<center> <img src="{{site.baseurl}}/images/graph.png" width="200"> </center>

Then we have a *transition matrix* $P$ which tells us the probability from going from one state to another

$$
P = \begin{pmatrix}
      0.2 & 0.8 \\
      0.5 & 0.5
    \end{pmatrix}
$$

Let $X_0=(p_0,p_1)$, where $p_0$ is the probability that it will be sunny on the first day in our process, and $p_1$ is the probability that it will be Rainy. For instance, if we know that it is Sunny on our first day, then we set $X_0 = (1,0)$. For $n \geq 1$, we set $X_n = X_{n-1} P^n$, which can be computed by diagonalizing $P$. $X_n$ will tell us how likely it is to be Sunny or Rainy on the $n$-th day.


## Random walks on a graph

coming soon...
